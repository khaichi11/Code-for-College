{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Latar Belakang**\n",
        "\n",
        "Jadi pada era digital ini, banyak sekali anak-anak yang sudah diberikan gadget oleh orang tuanya, berawal dari fenomena inilah akhirnya kita mengambil sebuah langkah untuk mengenalkan manfaat dari buah-buahan kepada anak-anak tersebut, terutama anak sd yang sudah mulai gemar membaca.\n",
        "\n",
        "Disini kami membuat sebuah aplikasi yang bernama Buah-Seru, dengan aplikasi ini anak-anak dapat mendeteksi buah dan setelah dideteksi akan muncul sebuah kuis yang akan diberikan kepada anak tersebut, misalnya anak tersebut mendeteksi apel maka akan tampil tiga pilihan, apel, jeruk, pisang, dan anak akan disuruh memilih dari ketiga opsi tersebut, benar maupun salah jawaban anak tersebut tetap akan diberikan info mengenai buah yang telah di scan tersebut.\n",
        "\n",
        "Dengan adanya aplikasi ini kami harap pemberian gadget oleh orangtua kepada anaknya tidak hanya digunakan untuk bermain game, melainkan digunakan juga untuk melakukan pembelajaran interaktif dan juga meningkatkan literasi dari anak tersebut melalui aplikasi Buah-Seru ini."
      ],
      "metadata": {
        "id": "xu8jX2KonOcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapan dataset sebelum di normalisasi\n",
        "Disini kami menggunakan dataset yang berjumlah 6600 gambar, masing-masing gambar yaitu gambar apel, pisang, dan jeruk terdiri dari 2200 dataset untuk setiap kelas tersebut.\n",
        "\n",
        "Karena awal gambar yang didapatkan ketika mengunduh dataset masih memiliki background akhirnya digunakanlah teknik grabcut yang berguna untuk mengeliminasi background sehingga yang tersisa akhrinya hanya datasetnya saja, dan gambar latar belakangnya hilang."
      ],
      "metadata": {
        "id": "YyxQh3vksH_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# menentukan folder sumber dan tujuan\n",
        "sumber = '../data4/img/train/apel2'\n",
        "tujuan = '../output/apel2'\n",
        "\n",
        "# melakukan looping secara rekursif melalui semua subfolder\n",
        "for root, _, files in os.walk(sumber):\n",
        "    for fname in files:\n",
        "        if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            continue\n",
        "\n",
        "        # path atau jalur\n",
        "        src_path = os.path.join(root, fname)\n",
        "        rel_path = os.path.relpath(src_path, sumber)\n",
        "        dest_path = os.path.join(tujuan, rel_path)\n",
        "\n",
        "        # membuat folder tujuan jika belum ada\n",
        "        os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "\n",
        "        # membaca gambar\n",
        "        img = cv2.imread(src_path)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # mendefinisikan rectangle awal (contoh: margin 10% dari tepi)\n",
        "        x = int(w * 0.1)\n",
        "        y = int(h * 0.1)\n",
        "        rw = int(w * 0.8)\n",
        "        rh = int(h * 0.8)\n",
        "        rect = (x, y, rw, rh)\n",
        "\n",
        "        # melakukan inisialisasi mask & model\n",
        "        mask = np.zeros((h, w), np.uint8)\n",
        "        bgdModel = np.zeros((1,65), np.float64)\n",
        "        fgdModel = np.zeros((1,65), np.float64)\n",
        "\n",
        "        # menjalankan GrabCut\n",
        "        cv2.grabCut(img, mask, rect, bgdModel, fgdModel, iterCount=5, mode=cv2.GC_INIT_WITH_RECT)\n",
        "\n",
        "        # membuat mask biner: 0 itu untuk background, 1 itu untuk foreground\n",
        "        mask2 = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n",
        "\n",
        "        # menerapkan mask ke gambar\n",
        "        segmented = img * mask2[:, :, np.newaxis]\n",
        "\n",
        "        # menyimpan hasil\n",
        "        cv2.imwrite(dest_path, segmented)\n",
        "        print(f'Processed: {rel_path}')\n"
      ],
      "metadata": {
        "id": "eEZ_gStytEXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kode - CNN\n",
        "\n",
        "Disini kami menggunakan CNN karena datasetnya berupa gambar, selain itu CNN adalah salah satu algoritma Neural Network yang tersedia di tensorflow lite sehingga memudahkan kita untuk melakukan pengembangan aplikasi mobile tanpa harus melakukan compile di cloud ataupun perangkat lainnya."
      ],
      "metadata": {
        "id": "Vx-WnxF0pMyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengimport library yang dibutuhkan"
      ],
      "metadata": {
        "id": "Atgvr0p7pw4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecj4zYc8l-0w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# digunaka untuk operasi sistem file seperti mencari path file\n",
        "import glob\n",
        "# ini digunakan untuk mencari file dengan format tertentu jpg misalnya\n",
        "import random\n",
        "# ini digunakan untuk mengacak data\n",
        "import numpy as np\n",
        "# digunakan untuk operasi array seperti vector dan matirks\n",
        "from PIL import Image\n",
        "# digunakan untuk memanipulasi gambar di python seperti merubah ukuran dsb\n",
        "import tensorflow as tf\n",
        "# framework machine learning yang digunakan\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "# mengimpor sub modul dari keras yaitu API yang ada di tensorflow untuk membuat layer, model ,dan callback(penjaga saat training)\n",
        "from sklearn.model_selection import train_test_split\n",
        "# untuk melakukan random split dan stratified split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# untuk menghitung dan menampilkan confussion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "#  untuk plotting confussion matrix\n",
        "from collections import Counter\n",
        "# menghitung frekuensi label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path untuk folder dataset & parameter"
      ],
      "metadata": {
        "id": "3FL-Jvh0qEEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPAT_DATA_TRAIN    = '../data5'\n",
        "UKURAN_BATCH         = 32\n",
        "EPOCHS               = 10\n",
        "LEARNING_RATE        = 1e-3\n",
        "UKURAN_GAMBAR        = (258, 320)\n",
        "VAL_SPLIT            = 0.2\n",
        "TEST_SPLIT           = 0.1\n",
        "SEED                 = 42"
      ],
      "metadata": {
        "id": "Hi9pwzVgmKnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "**TEMPAT_DATA_TRAIN =** '../data5' menunjuk ke folder di mana dataset tersimpan.\n",
        "\n",
        "**UKURAN_BATCH**       = 32 yaitu jumlah  sampel yang diproses dalam satu batch saat melakukan training (jumlah data yang dikirimkan untuk sekali proses training).\n",
        "\n",
        "**EPOCHS**               = 10 ini adalah nilai iterasi yang dilakukan oleh model untuk melakukan training, jadi batch tadi ada hubungannya dengan epoch, data yang akan di training tadi sudah dibagi ke dalam batch kemudian akan di lakukan proses training atau latih untuk model.\n",
        "\n",
        "**LEARNING_RATE**        = 1e-3 ini adalah nilai dari kecepatan pembelajaran yang digunakan oleh sebuah optimizer untuk mengubah bobot model pada setiap iterasi. Optimizer yang digunakan di sini adalah Adam atau adaptive moment estimation yaitu salah satu jenis optimizer dengan memanfaatkan gradien untuk menyesuaikan pembaruan bobot secara adaptif.\n",
        "\n",
        "\n",
        "**UKURAN_GAMBAR**        = (258, 320) ini adalah ukuran dari gambar untuk dilakukan resize setiap gambar sebelum akhirnya dimasukkan ke dalam model, tingginya 258 dan lebarnya 320.\n",
        "\n",
        "**VAL_SPLIT**            = 0.2 pemisahan data untuk validation, validation disini berguna untuk memantau performa model tiap epoch tanpa mempengaruhi test set.\n",
        "\n",
        "**TEST_SPLIT**           = 0.1 pemisahan data untu test, test ini dilakukan sekali saja untuk menilai performa akhir dari model tersebut.\n",
        "\n",
        "**SEED**                = 42 digunakan untuk melakukan pengacakan pada data."
      ],
      "metadata": {
        "id": "nOiEDb5dICkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mengumpulkan file dan label"
      ],
      "metadata": {
        "id": "aY8gEO48w8A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semua_file  = glob.glob(os.path.join(TEMPAT_DATA_TRAIN, '*', '*'))\n",
        "semua_label = [os.path.basename(os.path.dirname(f)) for f in semua_file]\n",
        "nama_kelas = sorted(set(semua_label))\n",
        "label_ke_index = {c:i for i,c in enumerate(nama_kelas)}\n",
        "list_index    = [label_ke_index[l] for l in semua_label]\n",
        "\n",
        "print(\"Total sampel:\", len(semua_file))\n",
        "print(\"Kelas:\", nama_kelas)\n",
        "print(\"Jumlah sampel per kelas:\", Counter(semua_label))"
      ],
      "metadata": {
        "id": "uWDBiTLaw9pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "Disini awalnya kita akan menggunakan modul glob untuk mencari semua file yang ada di dalam folder TEMPAT_DATA_TRAIN, setelah itu akan dilakukan Loop untuk setiap path atau jalur f di semua_label, setelah itu nama kelas akan diurutkan secara alfabetis menggunakan sorted, selanjutnya buat dictionary untuk memetakan setiap nama kelas ke sebuah indeks integer, lakukan loop lagi untuk setiap nama kelas l di semua_label untuk mengganti nama kelas tersebut dengan indeks numeriknya menurut data yang ada pada variabel lakel_ke_index."
      ],
      "metadata": {
        "id": "VpZtPtZ8MqrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menghitung ukuran pemisahan data"
      ],
      "metadata": {
        "id": "2kRSij6CqVTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = len(semua_file)\n",
        "n_test  = int(n_total * TEST_SPLIT)\n",
        "n_val   = int(n_total * VAL_SPLIT)\n",
        "print(f\"\\nPemisahan data yang akan dilakukan: Test={n_test} samples, Val={n_val} samples, Train={n_total - n_test - n_val} samples\")"
      ],
      "metadata": {
        "id": "PFmkruXzqdVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "**n_test**  = int(n_total * TEST_SPLIT) ini itu berarti dataset untuk testnya itu 10% dari total\n",
        "\n",
        "**n_val**   = int(n_total * VAL_SPLIT) ni itu berarti dataset untuk validationnya itu 20% dari total"
      ],
      "metadata": {
        "id": "FjdfQUkXOyKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melakukan stratified split dengan ukuran tetap"
      ],
      "metadata": {
        "id": "z6ytUMUKqgG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_file  = np.array(semua_file)\n",
        "label_label = np.array(list_index)\n",
        "\n",
        "# a) ambil test set\n",
        "f_temp, f_test, l_temp, l_test = train_test_split(\n",
        "    file_file, label_label,\n",
        "    test_size=n_test,\n",
        "    shuffle=True,\n",
        "    stratify=label_label,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "# b) dari sisa ambil val set\n",
        "f_train, f_val, l_train, l_val = train_test_split(\n",
        "    f_temp, l_temp,\n",
        "    test_size=n_val,\n",
        "    shuffle=True,\n",
        "    stratify=l_temp,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "train_file_file, train_idxs = f_train.tolist(), l_train.tolist()\n",
        "val_file_file,   val_idxs   = f_val.tolist(),   l_val.tolist()\n",
        "test_file_file,  test_idxs  = f_test.tolist(),  l_test.tolist()\n",
        "\n",
        "print(\"\\nSplit distributions:\")\n",
        "print(\" Train:\", Counter([nama_kelas[i] for i in train_idxs]))\n",
        "print(\" Val  :\", Counter([nama_kelas[i] for i in val_idxs]))\n",
        "print(\" Test :\", Counter([nama_kelas[i] for i in test_idxs]))"
      ],
      "metadata": {
        "id": "UIu3Mfh5qtty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "outputId": "4bad2a25-eb34-4529-e469-1c7086595a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-565fca3820a2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_file\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemua_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabel_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 3a) ambil test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m f_temp, f_test, l_temp, l_test = train_test_split(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "Pada bagian ini kode akan mengubah daftar dari path file dan daftar indeks label menjadi array numpy pada variabel **file_file** dan juga **label_label** agar dapat diproses oleh t**rain_test_split** nantinya. Disini kita akan membagi data menjadi dua bagian terlebih dahulu yaitu ada **n_test** dan sisa data yaitu **f_temp** dan **l_temp** yang pada kondisi ini belum terpakai. Selanjutnya yaitu pemilihan sampel untuk test set yang dilakukan secara acak tetapi tetap mempertahankan proporsi dari setiap kelas karena ada parameter **stratify=label_label** sehingga membuat label di test set akan terdistribusi secara merata.\n",
        "\n",
        "Sisa data yang belum terpakai tadi yaitu **f_temp** dan **l_temp** akan dipisah lagi menjadi validation set sebanyak **n_val** sampel dan sisanya digunakan untuk training set. Proses ini membuat stratifikasi label agar nantinya validation setnya itu memiliki komposisi yang seimbang. Setelah pemgbagian selesai dilakukan, semua array hasil split diubah kembali menjadi list python yaitu **train_file_file**, **val_file_file**, dan **test_file_file** untuk pathnya serta **train_idxs**, **val_idxs**, dan **test_idxs** untuk labelnya. Kalau dilihat disitu ada counter yang berguna untuk memverifikasi bahwa data train, validation, dan testnya sudah pas proporsinya."
      ],
      "metadata": {
        "id": "fWFZFWX_PBXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membuat tf.data.Dataset dengan melakukan preprocessing manual minmax"
      ],
      "metadata": {
        "id": "gxZl618zqxvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_and_preprocess(path, label):\n",
        "\n",
        "    def _py_load(path_str):\n",
        "        img = Image.open(path_str.decode('utf-8')).convert('RGB')\n",
        "\n",
        "        try:\n",
        "            resample_method = Image.Resampling.LANCZOS\n",
        "        except AttributeError:\n",
        "            resample_method = Image.LANCZOS\n",
        "        img = img.resize((UKURAN_GAMBAR [1], UKURAN_GAMBAR [0]), resample_method)\n",
        "        arr = np.array(img).astype(np.float32) / 255.0\n",
        "        return arr\n",
        "\n",
        "\n",
        "\n",
        "    img = tf.numpy_function(func=_py_load, inp=[path], Tout=tf.float32)\n",
        "\n",
        "    img.set_shape((UKURAN_GAMBAR [0], UKURAN_GAMBAR [1], 3))\n",
        "    one_hot = tf.one_hot(label, depth=len(nama_kelas))\n",
        "    return img, one_hot\n",
        "\n",
        "def prepare(file_file, label_label, shuffle=False, augment=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((file_file, label_label))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=1000, seed=SEED)\n",
        "    ds = ds.map(load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    if augment:\n",
        "        aug = tf.keras.Sequential([\n",
        "            layers.RandomFlip('horizontal'),\n",
        "            layers.RandomRotation(0.1),\n",
        "            layers.RandomZoom(0.1),\n",
        "        ])\n",
        "        ds = ds.map(lambda x, y: (aug(x), y), num_parallel_calls=AUTOTUNE)\n",
        "    return ds.batch(UKURAN_BATCH).prefetch(AUTOTUNE)\n",
        "\n",
        "train_ds = prepare(train_file_file, train_idxs, shuffle=True,  augment=True)\n",
        "val_ds   = prepare(val_file_file,   val_idxs,   shuffle=False, augment=False)\n",
        "test_ds  = prepare(test_file_file,  test_idxs,  shuffle=False, augment=False)"
      ],
      "metadata": {
        "id": "n6uVp34aq85h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "Di bagian kode ini disini kita membuat sebuah pipeline data (sebuah rangkaian dari tahapan yang ada pada proses di machine learing untuk menyiapkan data dari bentuk mentah hingga siap digunakan oleh model) menggunakan API tf,data agar pembacaan dan pemrosesan gambar dapat berjalan secara efisien dan terintegrasi langsung ke dalam alur TensorFlow. Awalnya konstanta dari AUTOTUNE=tf.data.AUTOTUNE menginstruksi tensorflow untuk menyesuaikan sendiri tingkat paralelisme pemanggilan fungsi dan prefetchingnya berdasarkan dari sumber daya yang tersedia.\n",
        "\n",
        "Selanjutnya fungsi load_and_preprocess(path, label) bertugas untuk memuat setiap file gambar dari disk atau ssd (tergantung penyimpanan pc masing-masing, disini saya menggunakan ssd) dan mengubahnya menjadi tensor siap pakai (disini tensor yang saya maksud adalah struktur data yang merepresentasikan array-multi dimensi), di dalam tensor tersebut ada tf.numpy_function yang akan memanggil fungsi python _py_load yang akan membuka gambar via pillow, lalu gambar akan dikonversi ke format RGB, setelah itu akan dilakukan resize ke dimensinya yaitu berupa (lebar, tinggi) sesuai dengan UKURAN_GAMBAR yang telah didefinisikan di atas dengan metode resampling LANCZOS (yaitu teknik interpolasi berbasi fungsi sinc yang umum digunakan untuk mengubah ukuran gambar dengan kualitas tinggi), meskipun gambarnya beresolusi 258x320 tapi pixelnya disini tetap memiliki rentang dari 0 sampai 255, jadi resolusi tidak masalah, setelah itu pixel akan diubah menjadi array numpy dan akan dilakukan normalisasi min-max melalui pembagian dengan nilai 255, dengan cara ini kita dapat memetakan nilai mulai dari rentang 0.0 sampai 1.0 tanpa mempengaruhi ukuran dari gambar tersebut sehingga nantinya pixel berada pada skala yang sama dan akan mempercepat proses konvergensi model. Tensor hasil normalisasi ini kemudian akan diberi shape atau ukuran secara eksplisit berupa (tinggi, lebar, 3), hal ini dilakukan agar TensorFlow dapat membangun graph dengan benar dan label integer dapat diubah menjadi vector one-hot sepanjang jumlah kelas (len(nama_kelas)).  \n",
        "\n",
        "Setelah itu fungsi prepare(file_file, label_label, shuffle=False, augment=False) menyusun objek tf.data.Dataset dari sepasang list path dan label, disini jika parameter dari shuffle=True, maka dataset akan diacak dengan buffer size 1000 dan seed yang ditetapkan sebelumnya, selanjutnya setiap elemen dipetakan ke load_and_preprocess, setelah itu jika augment=True maka akan diterapkan  transformasi augmentasi pada datasetnya lewat sebuah model keras. dan selanjutnya dataset dibagi per batch sesuai dengan ukuran dari batch yang telah diaturan sebelumnya lewat variabel UKURAN_BATCH, kemudian akan dilakukan prefetch agar data berikutnya sudah siap ketika GPU selesai memproses batch sebelumnya.\n",
        "\n",
        "setelah itu, maka tiga dataset akan disiapkan sesuai fungsinya masing masing, traind_ds dengan shuffle=True dan augment=True digunakan untuk training, lalu val_ds dan test_ds dibuat tanpa shuffle dan augmentasi agar performa konsisten dan tidak dipengaruhi oleh variasi augmentasi.\n"
      ],
      "metadata": {
        "id": "UuUeRHmLPF9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membangun dan melakukan compile untuk model"
      ],
      "metadata": {
        "id": "_DUU4ndtrFAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Input(shape=(*UKURAN_GAMBAR , 3)),\n",
        "    layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(len(nama_kelas), activation='softmax'),\n",
        "], name='buah_manual_preprocess_cnn')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OFdAMhR9rKC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "Jadi disini itu karena kita menggunakan deeplearning dan algoritmanya CNN yang ada pada tensorflow kita tidak bisa melakukan import seperti pada library biasanya, misalnya ketika kita menggunakan KNN kita hanya tinggal melakukan import dari scikit-learn, tapi di tensorflow ini kita menggunakan penyusunan layer dari keras untuk menyusun algoritma CNN-nya, jadi layer yang digunakan untuk menyusun model CNN-nya yaitu :\n",
        "\n",
        "1. Input layer\n",
        "2. Convolutional layer\n",
        "3. Max Pooling layer\n",
        "4. Convolutional layer\n",
        "5. Max Pooling layer\n",
        "6. GlobalAveragePooling\n",
        "7. Fully Connected (Dense) layer\n",
        "8. Dropout layer\n",
        "9. Output layer\n",
        "\n",
        "Prosesnya yaitu input akan menerima citra RGB berukuran 258x320 dan menyiapkan tensor berdimensi (258, 320, 3). Setelah itu, layer konvolusi pertama menerapkan 32 filter berukuran 3x3 dengan padding \"same\" yang kemudian diaktifkan melalui fungsi relu untuk menambahkan kemampuan belajar pola non-linier. Setelah dilakukannya ektrasi awal maka layer max-pooling pertama akan mengambil nilai maksimum dengan ukuran 2x2 sehingga mereduksi tinggi dan lebar pada fitur, tetapi masih tetap mempertahankan fitur-fitur pentingnya. Proses ekstraksi ini kemudian dilanjutkan oleh layer konvolusi kedua yang menggunakan 64 filter berukuran 3x3 dengan padding \"same\" dan relu dan diikuti juga oleh lapisan max-pooling kedua untuk menurunkan resolusi spasial lebih lanjut.\n",
        "\n",
        "\n",
        "Disini itu kita tidak melakukan flatten (suatu layer yang mengubah semua nilai piksel di peta fitur menjadi satu vektor panjang), tapi model ini memakai globalaveragepooling yang menghitung rata rata nilai pada tiap fitur sehingga dapat menghasilkan satu vektor per filter tanpa membuat jumlah dari parameternya menjadi terlalu banyak atau membengkak. Vektor tersebut kemudian akan diproses kembali oleh layer dense yang berukuran 128 neuron dengan aktivasi relu untuk menggabungkan fitur, lalu lapisan dropout akan mematikan 30% neuron secara acak di lapisan ini selama training. Setelah itu layer output akan memiliki jumlah neuron yang sama banyaknya dengan kelas dan juga menggunakan aktivasi softmax untuk mengonversi nilai mentah menjadi distribusi probabilitas prediksi.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DCdMDKX1PHX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melakukan callbacks dan training"
      ],
      "metadata": {
        "id": "OjS-7tNarOMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_list = [\n",
        "    callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    callbacks.ModelCheckpoint('model_terbaik.h5', monitor='val_accuracy', save_best_only=True),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks_list\n",
        ")"
      ],
      "metadata": {
        "id": "o1s48jZCrYit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "Mekanisme Callback :\n",
        "Jadi disini ada earlystopping untuk memantau apakah validation loss turun atau tidak selama tiga epoch berturut-turut, kalau turun maka akan dihentikan, disini juga ada kode untuk melakukan perbaikan ketika hal tersebut terjadi yaitu restore_best_weight=True. Ada sebuah checkpoint disini yang berguna untuk menyimpan bobot mode ke file model_terbaik.h5 setiap kali akurasi dari validari (val_accuracy) meningkat.\n",
        "\n",
        "modelfit digunakan untuk mengatur jumlah epoch, memberikan data train, memberikan data validasi, dan menggunakan callback."
      ],
      "metadata": {
        "id": "_8h_LhD2PIva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Melakukan evaluasi pada test set\n"
      ],
      "metadata": {
        "id": "OR46vFW5rZeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('model_terbaik.h5')\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f\"\\nAkurasi test: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "M-q5REk9rfeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n",
        "TensorFlow akan secara berurutan memberikan setiap batch dari test_ds ke model, menghitung loss (seberapa besar prediksi model berbeda dari label sebenarnya) dan akurasi (persentase prediksi yang benar), lalu mengembalikan nilai rata‐rata dari metrik ini di seluruh test set."
      ],
      "metadata": {
        "id": "bSj_55lfPJ5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confussion matrix untuk evaluasi model"
      ],
      "metadata": {
        "id": "D05oXPFXrhFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = [], []\n",
        "for x_batch, y_batch in test_ds:\n",
        "    probs = model.predict(x_batch, verbose=0)\n",
        "    y_true.extend(tf.argmax(y_batch, axis=1).numpy())\n",
        "    y_pred.extend(np.argmax(probs, axis=1))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nama_kelas)\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "disp.plot(ax=ax, xticks_rotation=90)\n",
        "plt.title('Confusion Matrix pada Test Set')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q21anuIUrm6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n"
      ],
      "metadata": {
        "id": "r0XHmCt9PK-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menyimpan model dan melakukan konversi ke tflite"
      ],
      "metadata": {
        "id": "X0gkT65crs-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('siscer_cnn_deteksi_buah.h5')\n",
        "tflite = tf.lite.TFLiteConverter.from_keras_model(model).convert()\n",
        "with open('siscer_cnn_deteksi_buah.tflite', 'wb') as f:\n",
        "    f.write(tflite)\n",
        "\n",
        "print(\"Selesai menyimpan model dengan format .h5 & .tflite\")"
      ],
      "metadata": {
        "id": "SIW-s2MnrvId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n"
      ],
      "metadata": {
        "id": "k7Q5wY4cPML9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Menyimpan label dari model"
      ],
      "metadata": {
        "id": "7AvrS3u4ugiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_FILE = 'class.txt'\n",
        "with open(CLASS_FILE, 'w') as f:\n",
        "    for cls in class_names:\n",
        "        f.write(f\"{cls}\\n\")\n",
        "print(f\"Saved {len(class_names)} labels to {CLASS_FILE}\")"
      ],
      "metadata": {
        "id": "D8ykTjZxunqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penjelasan\n"
      ],
      "metadata": {
        "id": "MWzqWLsEPNIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Melakukan pengujian terhadap model yang sudah di konversi"
      ],
      "metadata": {
        "id": "GSNyjZqNr31e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "KERAS_MODEL_PATH  = 'siscer_cnn_deteksi_buah.h5'\n",
        "TFLITE_MODEL_PATH = 'siscer_cnn_deteksi_buah.tflite'\n",
        "IMG_SIZE = (320, 258)\n",
        "\n",
        "with open('class.txt', 'r') as f:\n",
        "    class_names = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(path, img_size=IMG_SIZE):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = img.resize(img_size)\n",
        "    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
        "    return arr\n",
        "\n",
        "def predict_with_keras(model_path, img_paths):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    images = np.stack([load_and_preprocess_image(p) for p in img_paths], axis=0)\n",
        "    probs  = model.predict(images)\n",
        "    preds  = np.argmax(probs, axis=1)\n",
        "\n",
        "    for path, p, prob in zip(img_paths, preds, probs):\n",
        "        label = class_names[p] if p < len(class_names) else f\"Unknown({p})\"\n",
        "        print(f\"{os.path.basename(path)} → {label} ({prob[p]*100:.2f}%)\")\n",
        "\n",
        "\n",
        "def predict_with_tflite(model_path, img_paths):\n",
        "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details  = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    for path in img_paths:\n",
        "        img = load_and_preprocess_image(path)[None, ...].astype(np.float32)\n",
        "        interpreter.set_tensor(input_details[0]['index'], img)\n",
        "        interpreter.invoke()\n",
        "        probs = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "        pred  = np.argmax(probs)\n",
        "        label = class_names[pred] if pred < len(class_names) else f\"Unknown({pred})\"\n",
        "        print(f\"{os.path.basename(path)} → {label} ({probs[pred]*100:.2f}%)\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    test_images = [\n",
        "        '../coba/apel3.jpg',\n",
        "        '../coba/pisang.jpg',\n",
        "    ]\n",
        "\n",
        "    print(\"=== Hasil perkiran Keras .h5 ===\")\n",
        "    predict_with_keras(KERAS_MODEL_PATH, test_images)\n",
        "\n",
        "    print(\"\\n=== Hasil perkiraan TFLite .tflite ===\")\n",
        "    predict_with_tflite(TFLITE_MODEL_PATH, test_images)\n"
      ],
      "metadata": {
        "id": "NVX1pd6AtC_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}